{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import scipy as sc\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "matplotlib.rcParams.update({'font.size': 22})\n",
    "import operator\n",
    "import json\n",
    "import seaborn as sns\n",
    "import scipy.stats as sc\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tp = pd.read_csv('resources/yelp_academic_dataset_user.csv', iterator=True, chunksize=1000)\n",
    "df_users = pd.concat(tp, ignore_index=True)\n",
    "tp = pd.read_csv('resources/yelp_academic_dataset_business.csv', iterator=True, chunksize=1000)\n",
    "df_business = pd.concat(tp, ignore_index=True)\n",
    "tp = pd.read_csv('resources/yelp_academic_dataset_review.csv', iterator=True, chunksize=1000)\n",
    "df_reviews = pd.concat(tp, ignore_index=True)\n",
    "tp = pd.read_csv('resources/yelp_academic_dataset_checkin.csv', iterator=True, chunksize=1000) \n",
    "df_checkin = pd.concat(tp, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Merging the review, user and business table\n",
    "df_merge = pd.merge(df_reviews,df_users, on='user_id',how='inner')\n",
    "df_merge = pd.merge(df_merge,df_business,on='business_id',how='inner')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1: Degree Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# networkx graph where the nodes are the businesses and users and there is an edge between \n",
    "# a business and a user if that user has reviewed that business\n",
    "G=nx.Graph()\n",
    "for index, row in df_merge.iterrows():\n",
    "    G.add_edge((row.name_x,\"user\",row.user_id),(row.name_y,\"business\",row.business_id)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "degsUsers = {}\n",
    "degsBusiness = {}\n",
    "for n in G.nodes () :\n",
    "    deg = G.degree ( n )\n",
    "    if n[1] == \"user\":\n",
    "        if deg not in degsUsers :\n",
    "            degsUsers [ deg ] = 0\n",
    "        degsUsers [ deg ] += 1 \n",
    "    if n[1] == \"business\":\n",
    "        if deg not in degsBusiness :\n",
    "            degsBusiness [ deg ] = 0\n",
    "        degsBusiness [ deg ] += 1\n",
    "itemsB = sorted ( degsBusiness.items () )\n",
    "\n",
    "itemsU = sorted ( degsUsers.items () )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# plotting the business degree distribution on linear scale\n",
    "fig = plt.figure (figsize=(20,10))\n",
    "ax = fig.add_subplot (111)\n",
    "ax.plot ([ k for (k , v ) in itemsB ] , [ v for (k ,v ) in itemsB ])\n",
    "plt.ylim([0,2000])\n",
    "plt.title ( \" Business Degree Distribution \" )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# plotting the user degree distribution on linear scale\n",
    "fig = plt.figure (figsize=(20,10))\n",
    "ax = fig.add_subplot (111)\n",
    "ax.plot ([ k for (k , v ) in itemsU ] , [ v for (k ,v ) in itemsU ])\n",
    "plt.ylim([0,200])\n",
    "plt.title ( \" User Degree Distribution \" )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# plotting the business degree distribution on log scale\n",
    "fig = plt.figure (figsize=(20,10))\n",
    "ax = fig.add_subplot (111)\n",
    "ax.plot ([ k for (k , v ) in itemsB ] , [ v for (k ,v ) in itemsB ])\n",
    "ax.set_xscale ('log')\n",
    "ax.set_yscale ('log') \n",
    "plt.title ( \" Business Degree Distribution in Log Scale\" )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# plotting the user degree distribution on linear scale\n",
    "fig = plt.figure (figsize=(20,10))\n",
    "ax = fig.add_subplot (111)\n",
    "ax.plot ([ k for (k , v ) in itemsU ] , [ v for (k ,v ) in itemsU ])\n",
    "ax.set_xscale ( 'log')\n",
    "ax.set_yscale ( 'log') \n",
    "plt.title ( \" Users Degree Distribution in Log Scale\" )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. User and business degree distribution seems to follow a power law distribution when plotted on a linear scale. However on plotting both the data on log scale we can see that the business degree distribution first increases and then decreases- this shows its not a power low distribution. The user distribution almost fits a straight line, hence it can be modelled as a power law distribution.\n",
    "\n",
    "2. The distribution shows that that most of the nodes have a low degree and as the degree increases the number of nodes having that degree decreases exponentially.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2: PageRank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Using the networkx provided implementation of the pagerank algorithm\n",
    "# Networkx converts an undirected graphs to a directed graph \n",
    "# with two directed edges for each undirected edge.\n",
    "\n",
    "pRank = nx.pagerank(G, alpha=0.85)\n",
    "\n",
    "sorted_x = sorted(pRank.items(), key=lambda x: x[1] if x[0][1] ==\"business\" else 0  \n",
    "                  ,reverse=True)[:100]\n",
    "df_top_100 =  pd.DataFrame(data = None, columns = df_business.columns)\n",
    "for i in range (0,100):\n",
    "    df2 = (df_business[df_business['business_id'] == sorted_x[i][0][2]])\n",
    "    frames = [df_top_100, df2]\n",
    "    df_top_100 = pd.concat(frames)\n",
    "\n",
    "df_top_100[['name','stars','review_count','attributes_Price Range']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be noted that the top 100 businesses have a good rating and also have a huge no. of reviews, an interesting observation is that most businesses in this list are on the lower side of the price range probably because the lower the price the more people that can afford to try it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# write to json\n",
    "data={}\n",
    "for i in range(0,100):\n",
    "    key = \"Top\"+str(i + 1)\n",
    "    data[key] = {}\n",
    "    data[key]['pagerank'] =  sorted_x[i][1]\n",
    "    data[key]['businessId'] = sorted_x[i][0][2]\n",
    "\n",
    "with open('HW3.json', 'w') as outfile:\n",
    "    json.dump(data, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Question 3 : Ranking Features\n",
    "Factors which are used to rank businesses \n",
    "1. review count\n",
    "2. average rating\n",
    "3. number of checkins\n",
    "4. completeness of profile as measured by the count of Nan values for attributes\n",
    "5. frequency of reviews i.e number of reviews per year\n",
    "\n",
    "Price range coul also be a good ranking factor but it is not used since its values are discrete so many businesses fall in the same price range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# The stars column for business is not continuous so finding the average rating for business using \n",
    "# the review dataframe\n",
    "b_rating = df_merge['stars_x'].groupby(df_merge['business_id']).mean()\n",
    "df_rating = pd.DataFrame(list(b_rating.items()), columns=['business_id', 'average_rating'])\n",
    "df_business = pd.merge(df_business,df_rating,on='business_id',how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# count no. of NaN values per row\n",
    "df_business['fields_incomplete'] = df_business.isnull().sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# find the checkin count\n",
    "df_checkin['checkin_count'] = df_checkin.sum(axis = 1, skipna = True)\n",
    "df_checkin = df_checkin[['business_id','checkin_count']]\n",
    "df_business = pd.merge(df_business,df_checkin,on='business_id',how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# find the frequency of rating\n",
    "df_merge['date'] = pd.to_datetime(df_merge['date'])\n",
    "b_rating_freq = df_merge['date'].map( lambda x : x.year).groupby(df_merge['business_id'])\n",
    "b_rating_freq = b_rating_freq.agg(lambda x: x.value_counts().mean())\n",
    "df_rating_freq = pd.DataFrame(list(b_rating_freq.items()), \n",
    "                              columns=['business_id', 'rating_frequency'])\n",
    "df_business = pd.merge(df_business,df_rating_freq,on='business_id',how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# plotting the correlation matrix\n",
    "cols = ['average_rating','review_count','checkin_count','fields_incomplete','rating_frequency']\n",
    "correlations = df_business[cols].corr(method='spearman')\n",
    "\n",
    "fig = plt.figure(figsize=(20,10))\n",
    "ax = fig.add_subplot(111)\n",
    "cax = ax.matshow(correlations, vmin=-1, vmax=1)\n",
    "fig.colorbar(cax)\n",
    "ticks = np.arange(0,5,1)\n",
    "ax.set_xticks(ticks)\n",
    "ax.set_yticks(ticks)\n",
    "ax.set_xticklabels(cols)\n",
    "ax.set_yticklabels(cols)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# table showing the spearman correlation coefficient for all pairs of ranking features \n",
    "correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# permutation test\n",
    "for i in range(0,len(cols)):\n",
    "    for j in range(i+1,len(cols)):\n",
    "        result = correlations[cols[i]][cols[j]]\n",
    "        print(\"spearman coeff. b/w\",cols[i],\"&\",cols[j],\":\",result)\n",
    "        r_list = []\n",
    "        for k in range(0,10000):\n",
    "            r_list.append(sc.spearmanr(df_business[cols[i]], \n",
    "                                       df_business[cols[j]].reindex(\n",
    "                                        np.random.permutation(df_business.index)))[0])\n",
    "        if result > 0:\n",
    "            pvalue = len([l for l in r_list if l >= float(result)])/10000\n",
    "        else:\n",
    "            pvalue = len([l for l in r_list if l <= float(result)])/10000\n",
    "        \n",
    "        sns.distplot(r_list)\n",
    "        plt.axvline(x=result,ymax=0.7,color='g',linewidth=4)\n",
    "        plt.show()\n",
    "        print(\"\\n p-value calculated from permutation test: \" + str(pvalue) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pvalue calculated using the spearmanr correlation and the one from the permutation test are show above.\n",
    "Since the calculated pvalues for all pair of variables are lower than alpha, the level of significance of 0.05, the correlation between all the pair of variables are significant and not likely to have occured by chance and hence we can reject the null hypothesis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Question 4: Pagerank with aggregate ranking features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Normalize all the ranking features for calculating an aggregate score\n",
    "max = df_business['average_rating'].max()\n",
    "min = df_business['average_rating'].min()\n",
    "df_business['average_rating_N'] =  df_business['average_rating'].apply(\n",
    "                                            lambda x :(x - min)/(max - min), 1)\n",
    "\n",
    "max = df_business['review_count'].max()\n",
    "min = df_business['review_count'].min()\n",
    "df_business['review_count_N'] =  df_business['review_count'].apply( \n",
    "                                            lambda x :(x - min)/(max - min), 1)\n",
    "\n",
    "max = df_business['checkin_count'].max()\n",
    "min = df_business['checkin_count'].min()\n",
    "df_business['checkin_count_N'] =  df_business['checkin_count'].apply(\n",
    "                                            lambda x :(x - min)/(max - min), 1)\n",
    "\n",
    "max = df_business['fields_incomplete'].max()\n",
    "min = df_business['fields_incomplete'].min()\n",
    "df_business['fields_incomplete_N'] =  df_business['fields_incomplete'].apply(\n",
    "                                            lambda x :(x - min)/(max - min), 1)\n",
    "\n",
    "max = df_business['rating_frequency'].max()\n",
    "min = df_business['rating_frequency'].min()\n",
    "df_business['rating_frequency_N'] =  df_business['rating_frequency'].apply(\n",
    "                                            lambda x :(x - min)/(max - min), 1)\n",
    "\n",
    "df_business['business_score'] = df_business.apply(\n",
    "    lambda row: row['average_rating_N'] + row['review_count_N'] + row['checkin_count_N'] \n",
    "        + row['fields_incomplete_N'] + row['rating_frequency_N'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#calculate user score\n",
    "# the user score is based on the usefulness of votes, no. of fans\n",
    "# and no. of times the person was an elite user\n",
    "\n",
    "df_users['user_score'] = df_users.apply(lambda row: row['votes.useful'] + len(row['elite']) + \n",
    "                             row['fans'], axis=1)\n",
    "# normalize the score\n",
    "max = df_users['user_score'].max()\n",
    "min = df_users['user_score'].min()\n",
    "df_users['user_score'] = df_users['user_score'].apply( lambda x:(x - min)/(max - min), 1)\n",
    "df_users['user_score'] \n",
    "df_users = df_users[['name','user_score','user_id']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_merge = pd.merge(df_reviews,df_users, on='user_id',how='inner')\n",
    "df_merge = pd.merge(df_merge,df_business,on='business_id',how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "G=nx.Graph()\n",
    "for index, row in df_merge.iterrows():\n",
    "    G.add_edge((row.name_x,\"user\",row.user_id,row.user_score),(row.name_y,\"business\",\n",
    "                                                               row.business_id,row.business_score)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "div = df_merge['user_score'].sum() + df_merge['business_score'].sum() \n",
    "#convert the score to a probability\n",
    "personalize = dict((n, n[3]/div) for n in G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pRank_new = nx.pagerank(G, alpha=0.9,personalization=personalize)\n",
    "sorted_x_new = sorted(pRank_new.items(), key=lambda x: x[1] if x[0][1] ==\"business\" else 0 ,\n",
    "                      reverse=True)[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_top_100_new =  pd.DataFrame(data = None, columns = df_business.columns)\n",
    "for i in range (0,100):\n",
    "    df2 = (df_business[df_business['business_id'] == sorted_x_new[i][0][2]])\n",
    "    frames = [df_top_100_new, df2]\n",
    "    df_top_100_new = pd.concat(frames)\n",
    "\n",
    "df_top_100_new[['name','average_rating','checkin_count','rating_frequency','review_count',\n",
    "                'attributes_Price Range']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Yes the rankings do differ after incorporating edge weights in the graph.\n",
    "\n",
    "When the pagerank was run with a uniform distribution prior the top 100 businesses where the ones with the highest number of user ratings without taking into account the quality of those rating, while with the second approach the users were assigned a score based on their activity on yelp thereby assigning more weightage to reviews from elite users also the businesses were assigned a score which is a aggregation of different business properties so the top 100 businesses performed well in a range of metrics and not just the count of reviews\n",
    "For e.g. the average rating for the top1 business in the first case (Gangnam Asian BBQ Dining )is 4.69 whereas in the second case (Mon Ami Gabi) is only 4.14 but this business has a very high checkin count, rating frequency and review count and hence is a more well rounded business overall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
